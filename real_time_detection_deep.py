"""
æ·±å±‚CNNå®æ—¶æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿ
æ”¯æŒçŸ³å¤´ã€å‰ªåˆ€ã€å¸ƒæ‰‹åŠ¿çš„å®æ—¶æ£€æµ‹
"""

import cv2
import torch
import torch.nn as nn
import numpy as np
from torchvision import transforms
from PIL import Image
import time
import os
from collections import Counter

# å®šä¹‰ä¸è®­ç»ƒæ—¶ç›¸åŒçš„æ·±å±‚CNNæ¨¡å‹
class CNN(nn.Module):
    def __init__(self):
        super(CNN, self).__init__()
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout1 = nn.Dropout2d(0.25)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout2 = nn.Dropout2d(0.25)
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout3 = nn.Dropout2d(0.25)
        
        # ç¬¬å››ä¸ªå·ç§¯å—
        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(256)
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout4 = nn.Dropout2d(0.3)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(256 * 14 * 14, 512)
        self.bn_fc1 = nn.BatchNorm1d(512)
        self.dropout5 = nn.Dropout(0.5)
        
        self.fc2 = nn.Linear(512, 256)
        self.bn_fc2 = nn.BatchNorm1d(256)
        self.dropout6 = nn.Dropout(0.5)
        
        self.fc3 = nn.Linear(256, 128)
        self.bn_fc3 = nn.BatchNorm1d(128)
        self.dropout7 = nn.Dropout(0.3)
        
        self.fc4 = nn.Linear(128, 3)  # è¾“å‡ºä¸º3ç±»ï¼šçŸ³å¤´ã€å‰ªåˆ€ã€å¸ƒ

    def forward(self, x):
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))
        x = self.dropout1(x)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))
        x = self.dropout2(x)
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))
        x = self.dropout3(x)
        
        # ç¬¬å››ä¸ªå·ç§¯å—
        x = self.pool4(torch.relu(self.bn4(self.conv4(x))))
        x = self.dropout4(x)
        
        # å±•å¹³å¹¶é€šè¿‡å…¨è¿æ¥å±‚
        x = x.view(-1, 256 * 14 * 14)
        
        x = torch.relu(self.bn_fc1(self.fc1(x)))
        x = self.dropout5(x)
        
        x = torch.relu(self.bn_fc2(self.fc2(x)))
        x = self.dropout6(x)
        
        x = torch.relu(self.bn_fc3(self.fc3(x)))
        x = self.dropout7(x)
        
        x = self.fc4(x)
        return x

class DeepGestureDetector:
    def __init__(self, model_path='cnn_model.pth'):
        print("åˆå§‹åŒ–æ·±å±‚CNNæ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿ...")
        
        # ç±»åˆ«æ ‡ç­¾
        self.class_names = ['paper', 'rock', 'scissors']  # ImageFolderé»˜è®¤æ’åº
        self.class_names_cn = ['paper', 'rock', 'scissors']  # ä¸­æ–‡æ˜¾ç¤º
        
        # é¢„æµ‹å†å²è®°å½•ï¼Œç”¨äºæ—¶é—´å¹³æ»‘
        self.prediction_history = []
        self.history_size = 7  # å¢åŠ å†å²è®°å½•é•¿åº¦
        self.confidence_threshold = 0.75  # æé«˜ç½®ä¿¡åº¦é˜ˆå€¼
        
        # åˆå§‹åŒ–æ¨¡å‹
        self.model = CNN()
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        self.model.to(self.device)
        
        try:
            # ä¼˜å…ˆåŠ è½½æœ€ä½³æ¨¡å‹
            if os.path.exists('best_cnn_model.pth'):
                self.model.load_state_dict(torch.load('best_cnn_model.pth', map_location=self.device))
                print("âœ“ æœ€ä½³æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            else:
                self.model.load_state_dict(torch.load(model_path, map_location=self.device))
                print("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸï¼")
            
            self.model.eval()
            print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {self.device}")
            
        except FileNotFoundError:
            print(f"âŒ é”™è¯¯ï¼šæ‰¾ä¸åˆ°æ¨¡å‹æ–‡ä»¶ {model_path}")
            print("è¯·å…ˆè¿è¡Œ cnn.py è®­ç»ƒå¹¶ä¿å­˜æ¨¡å‹")
            return
        except Exception as e:
            print(f"âŒ æ¨¡å‹åŠ è½½é”™è¯¯: {e}")
            return
        
        # æ”¹è¿›çš„æ•°æ®é¢„å¤„ç†ï¼ˆä¸è®­ç»ƒæ—¶ä¿æŒä¸€è‡´ï¼‰
        self.transform = transforms.Compose([
            transforms.Resize((224, 224)),
            transforms.ToTensor(),
            transforms.Normalize(
                mean=[0.485, 0.456, 0.406], 
                std=[0.229, 0.224, 0.225]
            )
        ])
        
        # åˆå§‹åŒ–æ‘„åƒå¤´
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            print("âŒ é”™è¯¯ï¼šæ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return
        
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 800)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 600)
        self.cap.set(cv2.CAP_PROP_FPS, 30)
        
        print("âœ“ æ‘„åƒå¤´åˆå§‹åŒ–æˆåŠŸï¼")
        print("\nä½¿ç”¨è¯´æ˜ï¼š")
        print("- å°†æ‰‹åŠ¿æ”¾åœ¨ç»¿è‰²æ£€æµ‹æ¡†å†…")
        print("- ä¿æŒæ‰‹åŠ¿ç¨³å®š1-2ç§’è·å¾—æœ€ä½³è¯†åˆ«æ•ˆæœ")
        print("- æŒ‰ 'q' é€€å‡ºç¨‹åº")
        print("- æŒ‰ 's' ä¿å­˜å½“å‰å¸§")
    
    def preprocess_frame(self, frame):
        """æ”¹è¿›çš„é¢„å¤„ç†è§†é¢‘å¸§"""
        # åº”ç”¨é«˜æ–¯æ¨¡ç³Šå‡å°‘å™ªéŸ³
        frame = cv2.GaussianBlur(frame, (3, 3), 0)
        
        # è½¬æ¢ä¸ºPILå›¾åƒ
        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        
        # åº”ç”¨å˜æ¢
        tensor_image = self.transform(pil_image).unsqueeze(0).to(self.device)
        
        return tensor_image
    
    def predict_with_smoothing(self, frame):
        """å¸¦æ—¶é—´å¹³æ»‘çš„é¢„æµ‹"""
        # é¢„å¤„ç†å›¾åƒ
        input_tensor = self.preprocess_frame(frame)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1).cpu()
            confidence, predicted = torch.max(probabilities, 1)
            
            predicted_class = self.class_names[predicted.item()]
            confidence_score = confidence.item()
            
            # æ·»åŠ åˆ°å†å²è®°å½•
            self.prediction_history.append((predicted_class, confidence_score, probabilities[0].numpy()))
            
            # ä¿ç•™æœ€è¿‘çš„é¢„æµ‹ç»“æœ
            if len(self.prediction_history) > self.history_size:
                self.prediction_history.pop(0)
            
            # ä½¿ç”¨åŠ æƒå¹³å‡è¿›è¡Œæ—¶é—´å¹³æ»‘
            if len(self.prediction_history) >= 3:
                # æ ¹æ®å†å²è®°å½•é•¿åº¦åŠ¨æ€ç”Ÿæˆæƒé‡
                history_len = len(self.prediction_history)
                
                if history_len == 3:
                    weights = np.array([0.2, 0.3, 0.5])
                elif history_len == 4:
                    weights = np.array([0.1, 0.2, 0.3, 0.4])
                elif history_len == 5:
                    weights = np.array([0.1, 0.15, 0.2, 0.25, 0.3])
                elif history_len == 6:
                    weights = np.array([0.05, 0.1, 0.15, 0.2, 0.25, 0.25])
                else:  # history_len == 7
                    weights = np.array([0.05, 0.1, 0.1, 0.15, 0.2, 0.2, 0.2])
                
                # ç¡®ä¿æƒé‡æ•°ç»„é•¿åº¦ä¸å†å²è®°å½•åŒ¹é…
                weights = weights[:history_len]
                weights = weights / weights.sum()  # å½’ä¸€åŒ–æƒé‡
                
                avg_probs = np.average([item[2] for item in self.prediction_history], 
                                     weights=weights, axis=0)
                
                # æ‰¾åˆ°å¹³å‡æ¦‚ç‡æœ€é«˜çš„ç±»åˆ«
                smooth_predicted = np.argmax(avg_probs)
                smooth_confidence = avg_probs[smooth_predicted]
                smooth_class = self.class_names[smooth_predicted]
                
                return smooth_class, smooth_confidence, avg_probs
            
        return predicted_class, confidence_score, probabilities[0].numpy()
    
    def draw_detection_area(self, frame):
        """ç»˜åˆ¶æ£€æµ‹åŒºåŸŸ"""
        height, width = frame.shape[:2]
        
        # å®šä¹‰æ£€æµ‹åŒºåŸŸï¼ˆå³ä¸Šè§’ï¼Œæ›´å¤§çš„åŒºåŸŸï¼‰
        x1, y1 = width - 380, 50
        x2, y2 = width - 50, 380
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†ï¼ˆæ¸å˜è¾¹æ¡†æ•ˆæœï¼‰
        cv2.rectangle(frame, (x1-2, y1-2), (x2+2, y2+2), (0, 150, 0), 3)
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        
        # æ·»åŠ æ ‡é¢˜
        cv2.putText(frame, "Hand Gesture Area", (x1, y1-15), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        return frame[y1:y2, x1:x2]  # è¿”å›æ£€æµ‹åŒºåŸŸ
    
    def draw_ui(self, frame, gesture, confidence, probabilities):
        """ç»˜åˆ¶ç”¨æˆ·ç•Œé¢"""
        # ç»˜åˆ¶èƒŒæ™¯é¢æ¿
        overlay = frame.copy()
        cv2.rectangle(overlay, (10, 10), (350, 200), (0, 0, 0), -1)
        cv2.addWeighted(overlay, 0.7, frame, 0.3, 0, frame)
        
        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        if confidence > self.confidence_threshold:
            gesture_cn = self.class_names_cn[self.class_names.index(gesture)]
            result_text = f"{gesture_cn} ({gesture})"
            color = (0, 255, 0)  # ç»¿è‰²
            status = "è¯†åˆ«æˆåŠŸ"
        else:
            result_text = "æœªè¯†åˆ«"
            color = (0, 165, 255)  # æ©™è‰²
            status = "ç½®ä¿¡åº¦ä¸è¶³"
        
        # ä¸»è¦ç»“æœæ˜¾ç¤º
        cv2.putText(frame, result_text, (20, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)
        cv2.putText(frame, f"ç½®ä¿¡åº¦: {confidence:.1%}", (20, 80), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        cv2.putText(frame, f"çŠ¶æ€: {status}", (20, 105), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        y_offset = 135
        for i, (class_name, class_cn) in enumerate(zip(self.class_names, self.class_names_cn)):
            prob = probabilities[i]
            bar_width = int(prob * 200)
            
            # ç»˜åˆ¶æ¦‚ç‡æ¡
            cv2.rectangle(frame, (20, y_offset + i*20), (20 + bar_width, y_offset + i*20 + 15), 
                         (0, 255, 0) if i == np.argmax(probabilities) else (100, 100, 100), -1)
            cv2.rectangle(frame, (20, y_offset + i*20), (220, y_offset + i*20 + 15), 
                         (255, 255, 255), 1)
            
            # æ¦‚ç‡æ–‡æœ¬
            prob_text = f"{class_cn}: {prob:.1%}"
            cv2.putText(frame, prob_text, (230, y_offset + i*20 + 12), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 1)
        
        # æ˜¾ç¤ºå¸®åŠ©ä¿¡æ¯
        help_y = frame.shape[0] - 60
        cv2.putText(frame, "æŒ‰é”®: 'q'-é€€å‡º, 's'-ä¿å­˜æˆªå›¾", (20, help_y), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.6, (200, 200, 200), 2)
        cv2.putText(frame, f"FPS: {getattr(self, 'fps', 0):.1f}", (20, help_y + 25), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
    
    def run(self):
        """è¿è¡Œå®æ—¶æ£€æµ‹"""
        if not hasattr(self, 'cap') or not self.cap.isOpened():
            print("âŒ æ‘„åƒå¤´æœªæ­£ç¡®åˆå§‹åŒ–")
            return
        
        print("\nğŸš€ å¼€å§‹å®æ—¶æ‰‹åŠ¿è¯†åˆ«...")
        frame_count = 0
        start_time = time.time()
        
        try:
            while True:
                ret, frame = self.cap.read()
                if not ret:
                    print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢")
                    break
                
                # ç¿»è½¬å›¾åƒï¼ˆé•œåƒæ•ˆæœï¼Œè®©ç”¨æˆ·æ„Ÿè§‰æ›´è‡ªç„¶ï¼‰
                frame = cv2.flip(frame, 1)
                
                # ç»˜åˆ¶æ£€æµ‹åŒºåŸŸå¹¶è·å–ROI
                detection_roi = self.draw_detection_area(frame)
                
                try:
                    # é¢„æµ‹æ‰‹åŠ¿
                    gesture, confidence, probabilities = self.predict_with_smoothing(detection_roi)
                    
                    # ç»˜åˆ¶ç”¨æˆ·ç•Œé¢
                    self.draw_ui(frame, gesture, confidence, probabilities)
                    
                except Exception as e:
                    error_msg = f"æ£€æµ‹é”™è¯¯: {str(e)[:30]}"
                    print(f"âŒ {error_msg}")  # åœ¨æ§åˆ¶å°ä¹Ÿæ‰“å°é”™è¯¯
                    cv2.putText(frame, error_msg, (20, 50), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                    
                    # æ˜¾ç¤ºæ›´è¯¦ç»†çš„é”™è¯¯ä¿¡æ¯
                    cv2.putText(frame, "è¯·æ£€æŸ¥æ¨¡å‹æ–‡ä»¶æ˜¯å¦æ­£ç¡®", (20, 80), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1)
                
                # è®¡ç®—FPS
                frame_count += 1
                if frame_count % 30 == 0:
                    elapsed = time.time() - start_time
                    self.fps = 30 / elapsed if elapsed > 0 else 0
                    start_time = time.time()
                
                # æ˜¾ç¤ºç”»é¢
                cv2.imshow('æ·±å±‚CNNæ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿ', frame)
                
                # å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("ğŸ‘‹ ç”¨æˆ·é€€å‡ºç¨‹åº")
                    break
                elif key == ord('s'):
                    # ä¿å­˜æˆªå›¾
                    timestamp = time.strftime("%Y%m%d_%H%M%S")
                    filename = f"gesture_capture_{timestamp}.jpg"
                    cv2.imwrite(filename, frame)
                    print(f"ğŸ“¸ æˆªå›¾å·²ä¿å­˜: {filename}")
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        
        finally:
            # æ¸…ç†èµ„æº
            self.cap.release()
            cv2.destroyAllWindows()
            print("âœ“ èµ„æºæ¸…ç†å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 60)
    print("æ·±å±‚CNNå®æ—¶æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿ v2.0")
    print("æ”¯æŒï¼šçŸ³å¤´ã€å‰ªåˆ€ã€å¸ƒæ‰‹åŠ¿è¯†åˆ«")
    print("=" * 60)
    
    detector = DeepGestureDetector()
    if hasattr(detector, 'cap') and detector.cap.isOpened():
        detector.run()
    else:
        print("âŒ åˆå§‹åŒ–å¤±è´¥ï¼Œç¨‹åºé€€å‡º")

if __name__ == "__main__":
    main()
