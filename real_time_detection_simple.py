"""
ç®€åŒ–ç‰ˆå®æ—¶æ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿ
å…¼å®¹åŸç‰ˆå’Œæ·±å±‚CNNæ¨¡å‹
"""

import cv2
import torch
import torch.nn as nn
import numpy as np
from torchvision import transforms
from PIL import Image
import time
import os

# è‡ªåŠ¨æ£€æµ‹æ¨¡å‹ç»“æ„çš„CNNç±»
class AdaptiveCNN(nn.Module):
    def __init__(self):
        super(AdaptiveCNN, self).__init__()
        # è¿™é‡Œä¼šåœ¨åŠ è½½æ¨¡å‹æ—¶è‡ªåŠ¨é€‚é…ç»“æ„
        pass

# åŸç‰ˆç®€å•CNN
class SimpleCNN(nn.Module):
    def __init__(self):
        super(SimpleCNN, self).__init__()
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.fc1 = nn.Linear(64 * 56 * 56, 128)
        self.fc2 = nn.Linear(128, 3)

    def forward(self, x):
        x = self.pool(torch.relu(self.conv1(x)))
        x = self.pool(torch.relu(self.conv2(x)))
        x = x.view(-1, 64 * 56 * 56)
        x = torch.relu(self.fc1(x))
        x = self.fc2(x)
        return x

# æ·±å±‚CNN
class DeepCNN(nn.Module):
    def __init__(self):
        super(DeepCNN, self).__init__()
        
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, stride=1, padding=1)
        self.bn1 = nn.BatchNorm2d(32)
        self.pool1 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout1 = nn.Dropout2d(0.25)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1)
        self.bn2 = nn.BatchNorm2d(64)
        self.pool2 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout2 = nn.Dropout2d(0.25)
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)
        self.bn3 = nn.BatchNorm2d(128)
        self.pool3 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout3 = nn.Dropout2d(0.25)
        
        # ç¬¬å››ä¸ªå·ç§¯å—
        self.conv4 = nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)
        self.bn4 = nn.BatchNorm2d(256)
        self.pool4 = nn.MaxPool2d(kernel_size=2, stride=2)
        self.dropout4 = nn.Dropout2d(0.3)
        
        # å…¨è¿æ¥å±‚
        self.fc1 = nn.Linear(256 * 14 * 14, 512)
        self.bn_fc1 = nn.BatchNorm1d(512)
        self.dropout5 = nn.Dropout(0.5)
        
        self.fc2 = nn.Linear(512, 256)
        self.bn_fc2 = nn.BatchNorm1d(256)
        self.dropout6 = nn.Dropout(0.5)
        
        self.fc3 = nn.Linear(256, 128)
        self.bn_fc3 = nn.BatchNorm1d(128)
        self.dropout7 = nn.Dropout(0.3)
        
        self.fc4 = nn.Linear(128, 3)

    def forward(self, x):
        # ç¬¬ä¸€ä¸ªå·ç§¯å—
        x = self.pool1(torch.relu(self.bn1(self.conv1(x))))
        x = self.dropout1(x)
        
        # ç¬¬äºŒä¸ªå·ç§¯å—
        x = self.pool2(torch.relu(self.bn2(self.conv2(x))))
        x = self.dropout2(x)
        
        # ç¬¬ä¸‰ä¸ªå·ç§¯å—
        x = self.pool3(torch.relu(self.bn3(self.conv3(x))))
        x = self.dropout3(x)
        
        # ç¬¬å››ä¸ªå·ç§¯å—
        x = self.pool4(torch.relu(self.bn4(self.conv4(x))))
        x = self.dropout4(x)
        
        # å±•å¹³å¹¶é€šè¿‡å…¨è¿æ¥å±‚
        x = x.view(-1, 256 * 14 * 14)
        
        x = torch.relu(self.bn_fc1(self.fc1(x)))
        x = self.dropout5(x)
        
        x = torch.relu(self.bn_fc2(self.fc2(x)))
        x = self.dropout6(x)
        
        x = torch.relu(self.bn_fc3(self.fc3(x)))
        x = self.dropout7(x)
        
        x = self.fc4(x)
        return x

class SimpleGestureDetector:
    def __init__(self):
        print("åˆå§‹åŒ–å…¼å®¹ç‰ˆæ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿ...")
        
        # ç±»åˆ«æ ‡ç­¾
        self.class_names = ['paper', 'rock', 'scissors']
        
        # é¢„æµ‹å†å²è®°å½•
        self.prediction_history = []
        self.history_size = 5  # å‡å°‘å†å²è®°å½•é•¿åº¦
        
        # è®¾å¤‡
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
        print(f"âœ“ ä½¿ç”¨è®¾å¤‡: {self.device}")
        
        # å°è¯•åŠ è½½æ¨¡å‹
        self.model = None
        self.model_type = None
        
        # æŒ‰ä¼˜å…ˆçº§å°è¯•åŠ è½½ä¸åŒçš„æ¨¡å‹
        model_files = [
            ('best_cnn_model.pth', 'æœ€ä½³æ·±å±‚CNN'),
            ('cnn_model.pth', 'CNNæ¨¡å‹'),
        ]
        
        for model_file, model_desc in model_files:
            if os.path.exists(model_file):
                if self.try_load_model(model_file, model_desc):
                    break
        
        if self.model is None:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°å¯ç”¨çš„æ¨¡å‹æ–‡ä»¶!")
            print("è¯·å…ˆè¿è¡Œ cnn.py è®­ç»ƒæ¨¡å‹")
            return
        
        # æ•°æ®é¢„å¤„ç†
        if self.model_type == 'deep':
            # æ·±å±‚CNNä½¿ç”¨æ ‡å‡†åŒ–
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor(),
                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
            ])
        else:
            # ç®€å•CNNä¸ä½¿ç”¨æ ‡å‡†åŒ–
            self.transform = transforms.Compose([
                transforms.Resize((224, 224)),
                transforms.ToTensor()
            ])
        
        # åˆå§‹åŒ–æ‘„åƒå¤´
        self.cap = cv2.VideoCapture(0)
        if not self.cap.isOpened():
            print("âŒ é”™è¯¯ï¼šæ— æ³•æ‰“å¼€æ‘„åƒå¤´")
            return
        
        # è®¾ç½®æ‘„åƒå¤´å‚æ•°
        self.cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)
        self.cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)
        
        print("âœ“ æ‘„åƒå¤´åˆå§‹åŒ–æˆåŠŸï¼")
        print("\nä½¿ç”¨è¯´æ˜ï¼š")
        print("- å°†æ‰‹åŠ¿æ”¾åœ¨ç»¿è‰²æ£€æµ‹æ¡†å†…")
        print("- æŒ‰ 'q' é€€å‡ºç¨‹åº")
    
    def try_load_model(self, model_file, model_desc):
        """å°è¯•åŠ è½½æ¨¡å‹"""
        try:
            # å…ˆå°è¯•åŠ è½½ä¸ºæ·±å±‚CNN
            model = DeepCNN().to(self.device)
            model.load_state_dict(torch.load(model_file, map_location=self.device))
            model.eval()
            self.model = model
            self.model_type = 'deep'
            print(f"âœ“ {model_desc}åŠ è½½æˆåŠŸ (æ·±å±‚CNN)")
            return True
        except:
            try:
                # å¦‚æœå¤±è´¥ï¼Œå°è¯•åŠ è½½ä¸ºç®€å•CNN
                model = SimpleCNN().to(self.device)
                model.load_state_dict(torch.load(model_file, map_location=self.device))
                model.eval()
                self.model = model
                self.model_type = 'simple'
                print(f"âœ“ {model_desc}åŠ è½½æˆåŠŸ (ç®€å•CNN)")
                return True
            except Exception as e:
                print(f"âŒ åŠ è½½{model_desc}å¤±è´¥: {e}")
                return False
    
    def preprocess_frame(self, frame):
        """é¢„å¤„ç†è§†é¢‘å¸§"""
        # è½¬æ¢ä¸ºPILå›¾åƒ
        pil_image = Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))
        
        # åº”ç”¨å˜æ¢
        tensor_image = self.transform(pil_image).unsqueeze(0).to(self.device)
        
        return tensor_image
    
    def predict_simple(self, frame):
        """ç®€å•é¢„æµ‹ï¼Œæ— å¤æ‚å¹³æ»‘"""
        # é¢„å¤„ç†å›¾åƒ
        input_tensor = self.preprocess_frame(frame)
        
        # æ¨¡å‹æ¨ç†
        with torch.no_grad():
            outputs = self.model(input_tensor)
            probabilities = torch.softmax(outputs, dim=1).cpu().numpy()[0]
            predicted = np.argmax(probabilities)
            
            predicted_class = self.class_names[predicted]
            confidence = probabilities[predicted]
            
            # ç®€å•çš„å†å²è®°å½•å¹³æ»‘
            self.prediction_history.append(predicted_class)
            if len(self.prediction_history) > self.history_size:
                self.prediction_history.pop(0)
            
            # ä½¿ç”¨æœ€å¸¸è§çš„é¢„æµ‹ä½œä¸ºæœ€ç»ˆç»“æœ
            if len(self.prediction_history) >= 3:
                from collections import Counter
                most_common = Counter(self.prediction_history).most_common(1)[0]
                if most_common[1] >= 2:  # å¦‚æœæœ€å¸¸è§çš„é¢„æµ‹å‡ºç°è‡³å°‘2æ¬¡
                    predicted_class = most_common[0]
            
            return predicted_class, confidence, probabilities
    
    def draw_detection_area(self, frame):
        """ç»˜åˆ¶æ£€æµ‹åŒºåŸŸ"""
        height, width = frame.shape[:2]
        
        # å®šä¹‰æ£€æµ‹åŒºåŸŸ
        x1, y1 = width - 300, 50
        x2, y2 = width - 50, 300
        
        # ç»˜åˆ¶æ£€æµ‹æ¡†
        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)
        cv2.putText(frame, "Hand Area", (x1, y1-10), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)
        
        return frame[y1:y2, x1:x2]
    
    def draw_ui(self, frame, gesture, confidence, probabilities):
        """ç»˜åˆ¶ç”¨æˆ·ç•Œé¢"""
        # æ˜¾ç¤ºé¢„æµ‹ç»“æœ
        if confidence > 0.6:  # é™ä½é˜ˆå€¼
            result_text = f"{gesture}: {confidence:.2f}"
            color = (0, 255, 0)
        else:
            result_text = "æœªè¯†åˆ«"
            color = (0, 0, 255)
        
        cv2.putText(frame, result_text, (20, 50), 
                   cv2.FONT_HERSHEY_SIMPLEX, 1.2, color, 3)
        
        # æ˜¾ç¤ºæ‰€æœ‰ç±»åˆ«çš„æ¦‚ç‡
        y_offset = 100
        for i, class_name in enumerate(self.class_names):
            prob = probabilities[i]
            prob_text = f"{class_name}: {prob:.3f}"
            cv2.putText(frame, prob_text, (20, y_offset + i*30), 
                       cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)
        
        # æ˜¾ç¤ºæ¨¡å‹ç±»å‹
        cv2.putText(frame, f"Model: {self.model_type.upper()}", (20, frame.shape[0]-30), 
                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (200, 200, 200), 1)
    
    def run(self):
        """è¿è¡Œå®æ—¶æ£€æµ‹"""
        if self.model is None or not self.cap.isOpened():
            print("âŒ åˆå§‹åŒ–æœªå®Œæˆ")
            return
        
        print("\nğŸš€ å¼€å§‹å®æ—¶æ‰‹åŠ¿è¯†åˆ«...")
        
        try:
            while True:
                ret, frame = self.cap.read()
                if not ret:
                    print("âŒ æ— æ³•è¯»å–æ‘„åƒå¤´ç”»é¢")
                    break
                
                # ç¿»è½¬å›¾åƒ
                frame = cv2.flip(frame, 1)
                
                # ç»˜åˆ¶æ£€æµ‹åŒºåŸŸå¹¶è·å–ROI
                detection_roi = self.draw_detection_area(frame)
                
                try:
                    # é¢„æµ‹æ‰‹åŠ¿
                    gesture, confidence, probabilities = self.predict_simple(detection_roi)
                    
                    # ç»˜åˆ¶ç”¨æˆ·ç•Œé¢
                    self.draw_ui(frame, gesture, confidence, probabilities)
                    
                except Exception as e:
                    error_msg = f"é¢„æµ‹é”™è¯¯: {str(e)[:25]}"
                    print(f"âŒ {error_msg}")
                    cv2.putText(frame, error_msg, (20, 50), 
                               cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)
                
                # æ˜¾ç¤ºç”»é¢
                cv2.imshow('å…¼å®¹ç‰ˆæ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿ', frame)
                
                # å¤„ç†æŒ‰é”®
                key = cv2.waitKey(1) & 0xFF
                if key == ord('q'):
                    print("ğŸ‘‹ ç”¨æˆ·é€€å‡ºç¨‹åº")
                    break
        
        except KeyboardInterrupt:
            print("\nğŸ‘‹ ç¨‹åºè¢«ç”¨æˆ·ä¸­æ–­")
        
        finally:
            # æ¸…ç†èµ„æº
            self.cap.release()
            cv2.destroyAllWindows()
            print("âœ“ èµ„æºæ¸…ç†å®Œæˆ")

def main():
    """ä¸»å‡½æ•°"""
    print("=" * 50)
    print("å…¼å®¹ç‰ˆæ‰‹åŠ¿è¯†åˆ«ç³»ç»Ÿ")
    print("æ”¯æŒåŸç‰ˆå’Œæ·±å±‚CNNæ¨¡å‹")
    print("=" * 50)
    
    detector = SimpleGestureDetector()
    detector.run()

if __name__ == "__main__":
    main()
